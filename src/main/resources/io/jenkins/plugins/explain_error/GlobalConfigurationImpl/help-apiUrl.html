The API endpoint URL for your chosen AI provider.

<h4>Ollama</h4>
<p>For Ollama, specify the base URL where your Ollama service is running:</p>
<ul>
<li>Local instance: <code>http://localhost:11434</code></li>
<li>Remote instance: <code>http://YOUR_OLLAMA_HOST:11434</code></li>
</ul>
<p>Note: Ensure the Ollama service is running and accessible from your Jenkins instance.</p>

<h4>OpenAI</h4>
<p><strong>Leave this field empty</strong> for standard OpenAI API usage.</p>
<p><strong>Specify a custom URL</strong> only when using custom OpenAI proxies or enterprise endpoints.</p>

<h4>Google Gemini</h4>
<p><strong>Leave this field empty</strong> for standard Google AI usage.</p>
<p><strong>Specify a custom URL</strong> only when using custom Gemini proxies or enterprise endpoints.</p>

<h4>How it Works</h4>
<p>When left empty, LangChain4j automatically uses the official API endpoints. When specified, your custom URL will be used instead, enabling compatibility with alternative API providers that implement the same interface.</p>
